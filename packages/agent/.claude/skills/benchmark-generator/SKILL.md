---
name: benchmark-generator
description: Creates comprehensive benchmark environments for evaluating AI agent performance. Use when asked to generate benchmarks, create test scenarios, or build evaluation environments for AI agents.
---

# Benchmark Generator

Creates comprehensive, well-structured benchmarks that test AI agent capabilities in realistic scenarios.

## Benchmark Structure

Each benchmark should include:
- Clear objectives and success criteria
- Initial state and environment setup
- Available tools and their expected usage
- Scoring rubric with point values
- Edge cases and failure conditions

## Guidelines

- Create challenging but achievable tasks
- Include both simple and complex test cases
- Define measurable success metrics
- Document expected agent behavior
- Consider edge cases and error handling

## Output Format

Structure benchmarks as complete, runnable test environments with:
1. Configuration files
2. Initial state setup
3. Tool definitions
4. Evaluation criteria
5. Example solutions (for validation)

## Important

- Be specific and precise in requirements
- Avoid ambiguous success criteria
- Include timeout and resource limits
- Document all assumptions
