# @quaver/engine

Orchestrates end-to-end benchmark execution across isolated Daytona sandboxes with shared volume data exchange.

## Features

- **Two-Sandbox Isolation** - Agent and benchmark run in separate sandboxes
- **Shared Volume** - Data exchange via volume subpaths
- **Progress Callbacks** - Real-time phase notifications
- **Automatic Cleanup** - Resources cleaned up on success or failure
- **CLI Interface** - Quick testing via command line

## Installation

```bash
bun add @quaver/engine
```

## Quick Start

```typescript
import { runBenchmark } from "@quaver/engine/run";

const result = await runBenchmark({
  prompt: "Create a benchmark for testing API rate limiting",
  onProgress: (phase, message) => console.log(`[${phase}] ${message}`),
});

console.log(`Run ID: ${result.runId}`);
console.log(`Score: ${result.results.score}`);
```

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                      Benchmark Run Flow                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. Create Volume: benchmarks-{runId}                           │
│     ├── /agent/        (agent writes spec here)                 │
│     └── /benchmark/    (benchmark writes results here)          │
│                                                                 │
│  2. Agent Sandbox                                               │
│     └── Mounts volume at /output (subpath: agent/)              │
│     └── Writes benchmark spec to /output/spec.json              │
│                                                                 │
│  3. Benchmark Sandbox                                           │
│     └── Mounts volume at /input (subpath: agent/)               │
│     └── Mounts volume at /output (subpath: benchmark/)          │
│     └── Writes results to /output/results.json                  │
│                                                                 │
│  4. Collect Results & Cleanup                                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## API

### runBenchmark(config)

Main orchestration function:

```typescript
import { runBenchmark } from "@quaver/engine/run";
import type { RunConfig, RunResult } from "@quaver/engine/types";

const config: RunConfig = {
  prompt: "Your benchmark description",
  agentModel: "claude-sonnet-4-5",  // Optional
  benchmarkModel: "gpt-4o",          // Optional
  resources: {                       // Optional
    cpu: 2,
    memory: 4096,
    disk: 10240,
  },
  onProgress: (phase, message) => {
    console.log(`[${phase}] ${message}`);
  },
};

const result: RunResult = await runBenchmark(config);
```

## Types

### RunConfig

```typescript
type RunConfig = {
  prompt: string;                                    // Benchmark description
  agentModel?: string;                               // Model for agent phase
  benchmarkModel?: string;                           // Model for benchmark phase
  resources?: {
    cpu?: number;
    memory?: number;
    disk?: number;
  };
  onProgress?: (phase: Phase, message: string) => void;
};
```

### Phase

```typescript
type Phase = "setup" | "agent" | "benchmark" | "cleanup";
```

### BenchmarkSpec

Generated by the agent sandbox:

```typescript
type BenchmarkSpec = {
  name: string;
  description: string;
  tasks: Array<{
    id: string;
    prompt: string;
    expectedOutcome: string;
  }>;
};
```

### BenchmarkResults

Generated by the benchmark sandbox:

```typescript
type BenchmarkResults = {
  score: number;
  taskResults: Array<{
    taskId: string;
    passed: boolean;
    output: string;
    duration: number;
  }>;
};
```

### RunResult

Final output from `runBenchmark`:

```typescript
type RunResult = {
  runId: string;
  spec: BenchmarkSpec;
  results: BenchmarkResults;
  metrics: {
    agentDuration: number;      // ms
    benchmarkDuration: number;  // ms
    totalDuration: number;      // ms
  };
};
```

## CLI

Test the engine from command line:

```bash
bun run start "Create a benchmark for testing API rate limiting"
```

Output:

```
Starting benchmark run...

Prompt: Create a benchmark for testing API rate limiting

[setup] Creating shared volume...
[agent] Creating agent sandbox...
[agent] Running agent...
[benchmark] Creating benchmark sandbox...
[benchmark] Running benchmark...
[cleanup] Cleaning up...

=== Results ===

Run ID: run-m5x2k1-abc123
Score: 0.85

Metrics:
  Agent duration: 2341ms
  Benchmark duration: 1523ms
  Total duration: 4892ms

Spec: generated-benchmark
  Create a benchmark for testing API rate limiting

Task Results:
  - task-1: PASSED
```

## Execution Flow

1. **Setup Phase**
   - Generate unique run ID
   - Create shared volume with subpaths

2. **Agent Phase**
   - Create agent sandbox with `/output` mount
   - Run agent code to generate benchmark spec
   - Write spec to `/output/spec.json`
   - Stop sandbox

3. **Benchmark Phase**
   - Create benchmark sandbox with `/input` and `/output` mounts
   - Read spec from `/input/spec.json`
   - Execute benchmark tasks
   - Write results to `/output/results.json`

4. **Cleanup Phase**
   - Delete agent sandbox
   - Delete benchmark sandbox
   - Delete shared volume

## Dependencies

| Package | Description |
|---------|-------------|
| `@quaver/sandbox` | Sandbox lifecycle management |
| `@daytonaio/sdk` | Daytona platform SDK |

## Exports

| Import Path | Description |
|-------------|-------------|
| `@quaver/engine/run` | `runBenchmark` function |
| `@quaver/engine/types` | Type definitions |

## Development

```bash
# Build
bun run build

# Type check
bun run check-types

# Watch mode
bun run dev

# Run CLI
bun run start "your prompt"

# Lint
npx ultracite check

# Fix lint issues
npx ultracite fix
```

## License

Private - Internal use only
